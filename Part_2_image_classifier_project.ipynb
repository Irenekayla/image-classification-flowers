{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irenekayla/image-classification-flowers/blob/main/Part_2_image_classifier_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMzxj9magU5e"
      },
      "source": [
        "## AI Programming with Python Nanodegree: Image Classifier Project\n",
        "  - Do not make changes to the first 2 code cells, they are being used for setting up the `flowers` dataset and `cat_to_name.json`. Start writing code from third code cell onwards.\n",
        "  - To use this notebook: `File > Save a copy in Drive`\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EUKirN80Na6"
      },
      "source": [
        "### Code Explanation:\n",
        "\n",
        "- **Setting Up Flower Dataset:**\n",
        "  - `data_dir = './flowers'`: Defines the directory path for the flower dataset.\n",
        "  - `FLOWERS_DIR = Path(data_dir)`: Uses `Path` from `pathlib` for handling PosixPath.\n",
        "\n",
        "- **Downloading and Extracting Dataset:**\n",
        "  - `if not FLOWERS_DIR.is_dir()`: Checks if the dataset directory exists.\n",
        "    - `FLOWERS_DIR.mkdir(parents=True, exist_ok=True)`: Creates the directory if not present.\n",
        "  - `TARBALL = FLOWERS_DIR / \"flower_data.tar.gz\"`: Defines the tarball path.\n",
        "  - Downloads and extracts the dataset if not already present:\n",
        "    - `request = requests.get(...)`: Downloads the 'flower_data.tar.gz' file.\n",
        "    - `with open(TARBALL, \"wb\") as file_ref`: Writes the downloaded content to the tarball.\n",
        "    - `with tarfile.open(TARBALL, \"r\") as tar_ref`: Extracts the tarball contents to the dataset directory.\n",
        "\n",
        "- **Cleaning Up:**\n",
        "  - `os.remove(TARBALL)`: Deletes the downloaded tarball to save space.\n",
        "\n",
        "- **Status Messages:**\n",
        "  - Prints informative messages about the directory creation, download, extraction, and cleanup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIHswK_3zEM6",
        "outputId": "d9b4705b-42de-4e78-de53-130146643cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting colab-xterm\n",
            "  Downloading colab_xterm-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (6.3.3)\n",
            "Downloading colab_xterm-0.2.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: colab-xterm\n",
            "Successfully installed colab-xterm-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install colab-xterm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCqndaL1zNeL"
      },
      "outputs": [],
      "source": [
        "%load_ext colabxterm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As4YyLS_zRHL"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFHu1nUUE_PF",
        "outputId": "41c8f0e9-992d-49c5-ead1-60a09ba912bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Directory created: ./flowers\n",
            "\n",
            "[INFO] Downloading the file 'flower_data.tar.gz' to ./flowers\n",
            "[INFO] 'flower_data.tar.gz' saved to ./flowers\n",
            "\n",
            "[INFO] Extracting the downloaded tarball to ./flowers\n",
            "[INFO] 'flower_data.tar.gz' extracted successfully to ./flowers\n",
            "\n",
            "[INFO] Deleting the tarball to save space.\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import os\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "\n",
        "# defining dataset directory\n",
        "data_dir = './flowers'\n",
        "\n",
        "# using pathlib.Path for handling PosixPath\n",
        "FLOWERS_DIR = Path(data_dir)\n",
        "\n",
        "# downloading and setting up data if not already present\n",
        "if not FLOWERS_DIR.is_dir():\n",
        "    # creating directory\n",
        "    FLOWERS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[INFO] Directory created: ./{FLOWERS_DIR}\")\n",
        "\n",
        "    print() # for readability\n",
        "\n",
        "    # tarball path\n",
        "    TARBALL = FLOWERS_DIR / \"flower_data.tar.gz\"\n",
        "\n",
        "    # downloading and writing the tarball to './flowers' directory\n",
        "    print(f\"[INFO] Downloading the file 'flower_data.tar.gz' to ./{FLOWERS_DIR}\")\n",
        "    request = requests.get('https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz')\n",
        "    with open(TARBALL, \"wb\") as file_ref:\n",
        "        file_ref.write(request.content)\n",
        "        print(f\"[INFO] 'flower_data.tar.gz' saved to ./{FLOWERS_DIR}\")\n",
        "\n",
        "    print() # for readability\n",
        "\n",
        "    # extracting the downloaded tarball\n",
        "    print(f\"[INFO] Extracting the downloaded tarball to ./{FLOWERS_DIR}\")\n",
        "    with tarfile.open(TARBALL, \"r\") as tar_ref:\n",
        "        tar_ref.extractall(FLOWERS_DIR)\n",
        "        print(f\"[INFO] 'flower_data.tar.gz' extracted successfully to ./{FLOWERS_DIR}\")\n",
        "\n",
        "    print() # for readability\n",
        "\n",
        "    # using os.remove to delete the downloaded tarball\n",
        "    print(\"[INFO] Deleting the tarball to save space.\")\n",
        "    os.remove(TARBALL)\n",
        "else:\n",
        "    print(f\"[INFO] Dataset already setup at ./{FLOWERS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsRLKv2u0ard"
      },
      "source": [
        "### Code Explanation:\n",
        "\n",
        "- **Creating a JSON File for Flower Categories:**\n",
        "  - `data`: Defines a dictionary containing numerical keys and corresponding flower names.\n",
        "  - `with open('cat_to_name.json', 'w') as file`: Opens the file 'cat_to_name.json' for writing.\n",
        "  - `json.dump(data, file)`: Writes the dictionary data to the JSON file.\n",
        "\n",
        "- **Interpreting the Output:**\n",
        "  - The code creates a JSON file named 'cat_to_name.json' that serves as a mapping between numerical keys and flower names. This mapping can be useful for associating numerical labels with human-readable names in machine learning tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6378UAqFK_t"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "data = {\n",
        "    \"21\": \"fire lily\", \"3\": \"canterbury bells\", \"45\": \"bolero deep blue\", \"1\": \"pink primrose\", \"34\": \"mexican aster\",\n",
        "    \"27\": \"prince of wales feathers\", \"7\": \"moon orchid\", \"16\": \"globe-flower\", \"25\": \"grape hyacinth\", \"26\": \"corn poppy\",\n",
        "    \"79\": \"toad lily\", \"39\": \"siam tulip\", \"24\": \"red ginger\", \"67\": \"spring crocus\", \"35\": \"alpine sea holly\",\n",
        "    \"32\": \"garden phlox\", \"10\": \"globe thistle\", \"6\": \"tiger lily\", \"93\": \"ball moss\", \"33\": \"love in the mist\",\n",
        "    \"9\": \"monkshood\", \"102\": \"blackberry lily\", \"14\": \"spear thistle\", \"19\": \"balloon flower\", \"100\": \"blanket flower\",\n",
        "    \"13\": \"king protea\", \"49\": \"oxeye daisy\", \"15\": \"yellow iris\", \"61\": \"cautleya spicata\", \"31\": \"carnation\",\n",
        "    \"64\": \"silverbush\", \"68\": \"bearded iris\", \"63\": \"black-eyed susan\", \"69\": \"windflower\", \"62\": \"japanese anemone\",\n",
        "    \"20\": \"giant white arum lily\", \"38\": \"great masterwort\", \"4\": \"sweet pea\", \"86\": \"tree mallow\",\n",
        "    \"101\": \"trumpet creeper\", \"42\": \"daffodil\", \"22\": \"pincushion flower\", \"2\": \"hard-leaved pocket orchid\",\n",
        "    \"54\": \"sunflower\", \"66\": \"osteospermum\", \"70\": \"tree poppy\", \"85\": \"desert-rose\", \"99\": \"bromelia\", \"87\": \"magnolia\",\n",
        "    \"5\": \"english marigold\", \"92\": \"bee balm\", \"28\": \"stemless gentian\", \"97\": \"mallow\", \"57\": \"gaura\",\n",
        "    \"40\": \"lenten rose\", \"47\": \"marigold\", \"59\": \"orange dahlia\", \"48\": \"buttercup\", \"55\": \"pelargonium\",\n",
        "    \"36\": \"ruby-lipped cattleya\", \"91\": \"hippeastrum\", \"29\": \"artichoke\", \"71\": \"gazania\", \"90\": \"canna lily\",\n",
        "    \"18\": \"peruvian lily\", \"98\": \"mexican petunia\", \"8\": \"bird of paradise\", \"30\": \"sweet william\",\n",
        "    \"17\": \"purple coneflower\", \"52\": \"wild pansy\", \"84\": \"columbine\", \"12\": \"colt's foot\", \"11\": \"snapdragon\",\n",
        "    \"96\": \"camellia\", \"23\": \"fritillary\", \"50\": \"common dandelion\", \"44\": \"poinsettia\", \"53\": \"primula\",\n",
        "    \"72\": \"azalea\", \"65\": \"californian poppy\", \"80\": \"anthurium\", \"76\": \"morning glory\", \"37\": \"cape flower\",\n",
        "    \"56\": \"bishop of llandaff\", \"60\": \"pink-yellow dahlia\", \"82\": \"clematis\", \"58\": \"geranium\", \"75\": \"thorn apple\",\n",
        "    \"41\": \"barbeton daisy\", \"95\": \"bougainvillea\", \"43\": \"sword lily\", \"83\": \"hibiscus\", \"78\": \"lotus lotus\",\n",
        "    \"88\": \"cyclamen\", \"94\": \"foxglove\", \"81\": \"frangipani\", \"74\": \"rose\", \"89\": \"watercress\", \"73\": \"water lily\",\n",
        "    \"46\": \"wallflower\", \"77\": \"passion flower\", \"51\": \"petunia\"\n",
        "}\n",
        "\n",
        "with open('cat_to_name.json', 'w') as file:\n",
        "    json.dump(data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UeuVOhFkVU8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv9QMEFVqLyh",
        "outputId": "e0633b42-2b0b-4b36-d24c-3a4d80be43da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import nn, optim\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "\n",
        "def load_data(data_dir):\n",
        "  \"\"\"Loads the image dataset.\n",
        "\n",
        "  Args:\n",
        "    data_dir (str): Path to the directory containing the image dataset.\n",
        "\n",
        "  Returns:\n",
        "    dataloaders (dict): A dictionary containing the dataloaders for training, validation, and testing.\n",
        "    image_datasets (dict): A dictionary containing the image datasets for training, validation, and testing.\n",
        "    class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define data transformations\n",
        "  data_transforms = {\n",
        "      'train': transforms.Compose([\n",
        "          transforms.RandomRotation(30),\n",
        "          transforms.RandomResizedCrop(224),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ]),\n",
        "      'test': transforms.Compose([\n",
        "          transforms.Resize(256),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ]),\n",
        "      'valid': transforms.Compose([\n",
        "          transforms.Resize(256),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ]),\n",
        "  }\n",
        "\n",
        "  # Create image datasets\n",
        "  image_datasets = {\n",
        "      'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=data_transforms['train']),\n",
        "      'test': datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=data_transforms['test']),\n",
        "      'valid': datasets.ImageFolder(os.path.join(data_dir, 'valid'), transform=data_transforms['valid'])\n",
        "  }\n",
        "\n",
        "  # Create dataloaders\n",
        "  dataloaders = {\n",
        "      'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=64, shuffle=True),\n",
        "      'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=32, shuffle=True),\n",
        "      'valid': torch.utils.data.DataLoader(image_datasets['valid'], batch_size=32, shuffle=True)\n",
        "  }\n",
        "\n",
        "  # Get class to index mapping\n",
        "  class_to_idx = image_datasets['train'].class_to_idx\n",
        "\n",
        "  return dataloaders, image_datasets, class_to_idx\n",
        "\n",
        "\n",
        "def build_network(arch, hidden_units, drop_prob):\n",
        "  \"\"\"Builds the neural network.\n",
        "\n",
        "  Args:\n",
        "    arch (str): The name of the architecture to use.\n",
        "    hidden_units (int): The number of hidden units in the classifier.\n",
        "    drop_prob (float): The dropout probability.\n",
        "\n",
        "  Returns:\n",
        "    model (nn.Module): The built neural network.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the pretrained model\n",
        "  if arch == 'vgg13':\n",
        "    model = models.vgg13(pretrained=True)\n",
        "  elif arch == 'resnet50':\n",
        "    model = models.resnet50(pretrained=True)\n",
        "  else:\n",
        "    raise ValueError('Invalid architecture. Choose from vgg13 or resnet50.')\n",
        "\n",
        "  # Freeze the pretrained model's parameters\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Replace the classifier with a custom one\n",
        "  classifier = nn.Sequential(OrderedDict([\n",
        "      ('fc1', nn.Linear(model.classifier[0].in_features, hidden_units)),\n",
        "      ('relu1', nn.ReLU()),\n",
        "      ('dropout1', nn.Dropout(p=drop_prob)),\n",
        "      ('fc2', nn.Linear(hidden_units, 102)),\n",
        "      ('output', nn.LogSoftmax(dim=1))\n",
        "  ]))\n",
        "\n",
        "  model.classifier = classifier\n",
        "\n",
        "  # Move the model to the GPU if available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, epochs=5):\n",
        "  \"\"\"Trains the model.\n",
        "\n",
        "  Args:\n",
        "    model (nn.Module): The neural network model.\n",
        "    dataloaders (dict): A dictionary containing the dataloaders for training, validation, and testing.\n",
        "    criterion (nn.Module): The loss function.\n",
        "    optimizer (torch.optim): The optimizer.\n",
        "    epochs (int): The number of epochs to train for.\n",
        "\n",
        "  Returns:\n",
        "    model (nn.Module): The trained model.\n",
        "  \"\"\"\n",
        "\n",
        "  # Track the best validation accuracy\n",
        "  best_acc = 0.0\n",
        "\n",
        "  # Iterate over epochs\n",
        "  for epoch in range(epochs):\n",
        "    print('Epoch {}/{}'.format(epoch+1, epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Iterate over training and validation phases\n",
        "    for phase in ['train', 'valid']:\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "      else:\n",
        "        model.eval()\n",
        "\n",
        "      # Track running loss and accuracy\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      # Iterate over data batches\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero out the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update running loss and accuracy\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "      # Calculate epoch loss and accuracy\n",
        "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "      epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "      # Deep copy the model if better validation accuracy is found\n",
        "      if phase == 'valid' and epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "  # Load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model\n",
        "\n",
        "\n",
        "def save_checkpoint(model, save_dir, arch, hidden_units, drop_prob, class_to_idx):\n",
        "  \"\"\"Saves the checkpoint.\n",
        "\n",
        "  Args:\n",
        "    model (nn.Module): The trained model.\n",
        "    save_dir (str): The directory to save the checkpoint in.\n",
        "    arch (str): The name of the architecture.\n",
        "    hidden_units (int): The number of hidden units in the classifier.\n",
        "    drop_prob (float): The dropout probability.\n",
        "    class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create the checkpoint dictionary\n",
        "  checkpoint = {\n",
        "      'arch': arch,\n",
        "      'hidden_units': hidden_units,\n",
        "      'drop_prob': drop_prob,\n",
        "      'class_to_idx': class_to_idx,\n",
        "      'state_dict': model.state_dict()\n",
        "  }\n",
        "\n",
        "  # Save the checkpoint\n",
        "  torch.save(checkpoint, os.path.join(save_dir, 'checkpoint.pth'))\n",
        "\n",
        "def load_checkpoint(filepath):\n",
        "  \"\"\"Loads the checkpoint.\n",
        "\n",
        "  Args:\n",
        "    filepath (str): The path to the checkpoint file.\n",
        "\n",
        "  Returns:\n",
        "    model (nn.Module): The loaded model.\n",
        "    class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the checkpoint\n",
        "  checkpoint = torch.load(filepath)\n",
        "\n",
        "  # Build the network\n",
        "  model = build_network(checkpoint['arch'], checkpoint['hidden_units'], checkpoint['drop_prob'])\n",
        "\n",
        "  # Load the state dictionary\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "  # Set the class to index mapping\n",
        "  model.class_to_idx = checkpoint['class_to_idx']\n",
        "\n",
        "  return model, checkpoint['class_to_idx']\n",
        "\n",
        "\n",
        "def process_image(image_path):\n",
        "  \"\"\"Processes an image for prediction.\n",
        "\n",
        "  Args:\n",
        "    image_path (str): The path to the image file.\n",
        "\n",
        "  Returns:\n",
        "    img (torch.Tensor): The processed image as a PyTorch tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the image\n",
        "  img = Image.open(image_path)\n",
        "\n",
        "  # Preprocess the image\n",
        "  img = data_transforms['test'](img)\n",
        "\n",
        "  # Convert the image to a PyTorch tensor\n",
        "  img = torch.from_numpy(img).float()\n",
        "\n",
        "  # Add a batch dimension\n",
        "  img = img.unsqueeze(0)\n",
        "\n",
        "  # Move the image to the GPU if available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  img = img.to(device)\n",
        "\n",
        "  return img\n",
        "\n",
        "def predict(image_path, model, topk=5):\n",
        "  \"\"\"Predicts the class of an image.\n",
        "\n",
        "  Args:\n",
        "    image_path (str): The path to the image file.\n",
        "    model (nn.Module): The trained model.\n",
        "    topk (int): The number of top predictions to return.\n",
        "\n",
        "  Returns:\n",
        "    probs (list): A list of probabilities for the top predictions.\n",
        "    classes (list): A list of class indices for the top predictions.\n",
        "  \"\"\"\n",
        "\n",
        "  # Process the image\n",
        "  img = process_image(image_path)\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Make the prediction\n",
        "  with torch.no_grad():\n",
        "    output = model(img)\n",
        "\n",
        "  # Calculate the probabilities and class indices\n",
        "  probs, classes = torch.exp(output).topk(topk)\n",
        "  probs = probs.cpu().numpy()[0]\n",
        "  classes = classes.cpu().numpy()[0]\n",
        "\n",
        "  return probs, classes\n",
        "\n",
        "def predict_image(image_path, model, class_to_idx, topk=5, category_names=None):\n",
        "  \"\"\"Predicts the class of an image and prints the results.\n",
        "\n",
        "  Args:\n",
        "    image_path (str): The path to the image file.\n",
        "    model (nn.Module): The trained model.\n",
        "    class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "    topk (int): The number of top predictions to return.\n",
        "    category_names (dict): A dictionary mapping class indices to category names.\n",
        "  \"\"\"\n",
        "\n",
        "  # Predict the class\n",
        "  probs, classes = predict(image_path, model, topk)\n",
        "\n",
        "  # Get the class names\n",
        "  class_names = [idx_to_class[class_] for class_ in classes]\n",
        "\n",
        "  # Print the results\n",
        "  print('Top {} predictions:'.format(topk))\n",
        "  for prob, class_name in zip(probs, class_names):\n",
        "    print('  {}: {:.4f}'.format(class_name, prob))\n",
        "\n",
        "  # If category_names is provided, print the category names\n",
        "  if category_names:\n",
        "    print('\\nCategory names:')\n",
        "    for class_name in class_names:\n",
        "      print('  {}'.format(category_names[class_name]))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Train or predict flower names')\n",
        "    subparsers = parser.add_subparsers(dest='command')\n",
        "\n",
        "    # Train command\n",
        "    train_parser = subparsers.add_parser('train', help='Train a new model')\n",
        "    train_parser.add_argument('data_dir', help='Path to the image data directory')\n",
        "    train_parser.add_argument('--save_dir', default='.', help='Directory to save the checkpoint')\n",
        "    train_parser.add_argument('--arch', default='resnet50', choices=['vgg13', 'resnet50'], help='Model architecture')\n",
        "    train_parser.add_argument('--learning_rate', default=0.001, type=float, help='Learning rate')\n",
        "    train_parser.add_argument('--hidden_units', default=512, type=int, help='Number of hidden units')\n",
        "    train_parser.add_argument('--epochs', default=5, type=int, help='Number of epochs')\n",
        "    train_parser.add_argument('--gpu', action='store_true', help='Use GPU for training')\n",
        "\n",
        "    # Predict command\n",
        "    predict_parser = subparsers.add_parser('predict', help='Predict flower name from an image')\n",
        "    predict_parser.add_argument('image_path', help='Path to the image file')\n",
        "    predict_parser.add_argument('checkpoint', help='Path to the checkpoint file')\n",
        "    predict_parser.add_argument('--top_k', default=5, type=int, help='Number of top predictions to return')\n",
        "    predict_parser.add_argument('--category_names', default=None, help='Path to a JSON file containing category names')\n",
        "    predict_parser.add_argument('--gpu', action='store_true', help='Use GPU for inference')\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Set the device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.gpu else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if args.command == 'train':\n",
        "        # Load the data\n",
        "        dataloaders, image_datasets, class_to_idx = load_data(args.data_dir)\n",
        "\n",
        "        # Build the network\n",
        "        model = build_network(args.arch, args.hidden_units, 0.1)\n",
        "\n",
        "        # Define the loss function and optimizer\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.classifier.parameters(), lr=args.learning_rate)\n",
        "\n",
        "        # Train the model\n",
        "        model = train_model(model, dataloaders, criterion, optimizer, epochs=args.epochs)\n",
        "\n",
        "        # Save the checkpoint\n",
        "        save_checkpoint(model, args.save_dir, args.arch, args.hidden_units, 0.1, class_to_idx)\n",
        "\n",
        "        print('Model trained and saved.')\n",
        "\n",
        "    elif args.command == 'predict':\n",
        "        # Load the checkpoint\n",
        "        model, class_to_idx = load_checkpoint(args.checkpoint)\n",
        "\n",
        "        # Load the category names if provided\n",
        "        category_names = None\n",
        "        if args.category_names:\n",
        "            with open(args.category_names, 'r') as f:\n",
        "                category_names = json.load(f)\n",
        "\n",
        "        # Predict the image\n",
        "        predict_image(args.image_path, model, class_to_idx, topk=args.top_k, category_names=category_names)\n",
        "\n",
        "    else:\n",
        "        print('Invalid command. Use \"train\" or \"predict\".')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tr8pFV_jrtP",
        "outputId": "3a46e995-b168-4bfd-8616-c808fc9581cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import nn, optim\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# Define data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Define function to load data\n",
        "def load_data(data_dir):\n",
        "    \"\"\"Loads the image dataset.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to the directory containing the image dataset.\n",
        "\n",
        "    Returns:\n",
        "        dataloaders (dict): A dictionary containing the dataloaders for training, validation, and testing.\n",
        "        image_datasets (dict): A dictionary containing the image datasets for training, validation, and testing.\n",
        "        class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create image datasets\n",
        "    image_datasets = {\n",
        "        'train': datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=data_transforms['train']),\n",
        "        'test': datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=data_transforms['test']),\n",
        "        'valid': datasets.ImageFolder(os.path.join(data_dir, 'valid'), transform=data_transforms['valid'])\n",
        "    }\n",
        "\n",
        "    # Create dataloaders\n",
        "    dataloaders = {\n",
        "        'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=64, shuffle=True),\n",
        "        'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=32, shuffle=True),\n",
        "        'valid': torch.utils.data.DataLoader(image_datasets['valid'], batch_size=32, shuffle=True)\n",
        "    }\n",
        "\n",
        "    # Get class to index mapping\n",
        "    class_to_idx = image_datasets['train'].class_to_idx\n",
        "\n",
        "    return dataloaders, image_datasets, class_to_idx\n",
        "\n",
        "# Define function to build the network\n",
        "def build_network(arch, hidden_units, drop_prob):\n",
        "    \"\"\"Builds the neural network.\n",
        "\n",
        "    Args:\n",
        "        arch (str): The name of the architecture to use.\n",
        "        hidden_units (int): The number of hidden units in the classifier.\n",
        "        drop_prob (float): The dropout probability.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): The built neural network.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the pretrained model\n",
        "    if arch == 'vgg13':\n",
        "        model = models.vgg13(pretrained=True)\n",
        "    elif arch == 'resnet50':\n",
        "        model = models.resnet50(pretrained=True)\n",
        "    else:\n",
        "        raise ValueError('Invalid architecture. Choose from vgg13 or resnet50.')\n",
        "\n",
        "    # Freeze the pretrained model's parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the classifier with a custom one\n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "        ('fc1', nn.Linear(model.classifier[0].in_features, hidden_units)),\n",
        "        ('relu1', nn.ReLU()),\n",
        "        ('dropout1', nn.Dropout(p=drop_prob)),\n",
        "        ('fc2', nn.Linear(hidden_units, 102)),\n",
        "        ('output', nn.LogSoftmax(dim=1))\n",
        "    ]))\n",
        "\n",
        "    model.classifier = classifier\n",
        "\n",
        "    # Move the model to the GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define function to train the model\n",
        "def train_model(model, dataloaders, criterion, optimizer, epochs=5):\n",
        "    \"\"\"Trains the model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model.\n",
        "        dataloaders (dict): A dictionary containing the dataloaders for training, validation, and testing.\n",
        "        criterion (nn.Module): The loss function.\n",
        "        optimizer (torch.optim): The optimizer.\n",
        "        epochs (int): The number of epochs to train for.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): The trained model.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Track the best validation accuracy\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Iterate over epochs\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Iterate over training and validation phases\n",
        "        for phase in ['train', 'valid']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            # Track running loss and accuracy\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data batches\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero out the gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # Backward pass and optimization\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Update running loss and accuracy\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Calculate epoch loss and accuracy\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Deep copy the model if better validation accuracy is found\n",
        "            if phase == 'valid' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Define function to save the checkpoint\n",
        "def save_checkpoint(model, save_dir, arch, hidden_units, drop_prob, class_to_idx):\n",
        "    \"\"\"Saves the checkpoint.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained model.\n",
        "        save_dir (str): The directory to save the checkpoint in.\n",
        "        arch (str): The name of the architecture.\n",
        "        hidden_units (int): The number of hidden units in the classifier.\n",
        "        drop_prob (float): The dropout probability.\n",
        "        class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the checkpoint dictionary\n",
        "    checkpoint = {\n",
        "        'arch': arch,\n",
        "        'hidden_units': hidden_units,\n",
        "        'drop_prob': drop_prob,\n",
        "        'class_to_idx': class_to_idx,\n",
        "        'state_dict': model.state_dict()\n",
        "    }\n",
        "\n",
        "    # Save the checkpoint\n",
        "    torch.save(checkpoint, os.path.join(save_dir, 'checkpoint.pth'))\n",
        "\n",
        "# Define function to parse command line arguments\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Train a flower image classifier')\n",
        "    parser.add_argument('data_dir', help='Path to the image data directory')\n",
        "    parser.add_argument('--save_dir', default='.', help='Directory to save the checkpoint')\n",
        "    parser.add_argument('--arch', default='resnet50', choices=['vgg13', 'resnet50'], help='Model architecture')\n",
        "    parser.add_argument('--learning_rate', default=0.001, type=float, help='Learning rate')\n",
        "    parser.add_argument('--hidden_units', default=512, type=int, help='Number of hidden units')\n",
        "    parser.add_argument('--epochs', default=5, type=int, help='Number of epochs')\n",
        "    parser.add_argument('--drop_prob', default=0.1, type=float, help='Dropout probability')  # Add this line\n",
        "    parser.add_argument('--gpu', action='store_true', help='Use GPU for training')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "# Define main function\n",
        "def main():\n",
        "    # Parse command line arguments\n",
        "    args = parse_args()\n",
        "\n",
        "    # Set the device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.gpu else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load the data\n",
        "    dataloaders, image_datasets, class_to_idx = load_data(args.data_dir)\n",
        "\n",
        "    # Build the network\n",
        "    model = build_network(args.arch, args.hidden_units,  args.drop_prob)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model(model, dataloaders, criterion, optimizer, epochs=args.epochs)\n",
        "\n",
        "    # Save the checkpoint\n",
        "    save_checkpoint(model, args.save_dir, args.arch, args.hidden_units, 0.1, class_to_idx)\n",
        "\n",
        "    print('Model trained and saved.')\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YxfmVO9Yqhdb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwakgS9pkDwu",
        "outputId": "943ecae4-8d7e-4399-97c3-29fa47137a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict.py\n",
        "import argparse\n",
        "import json\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import nn, optim\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Define data transformations\n",
        "data_transforms = {\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Define function to load checkpoint\n",
        "def load_checkpoint(filepath):\n",
        "    \"\"\"Loads the checkpoint.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the checkpoint file.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): The loaded model.\n",
        "        class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "    \"\"\"\n",
        "\n",
        "    checkpoint = torch.load(filepath, map_location=torch.device('cpu'), weights_only=True)\n",
        "\n",
        "    # Build the network\n",
        "    model = build_network(checkpoint['arch'], checkpoint['hidden_units'], checkpoint['drop_prob'])\n",
        "\n",
        "    # Load the state dictionary\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    # Set the class to index mapping\n",
        "    model.class_to_idx = checkpoint['class_to_idx']\n",
        "\n",
        "    return model, checkpoint['class_to_idx']\n",
        "\n",
        "# Define function to build the network\n",
        "def build_network(arch, hidden_units, drop_prob):\n",
        "    \"\"\"Builds the neural network.\n",
        "\n",
        "    Args:\n",
        "        arch (str): The name of the architecture to use.\n",
        "        hidden_units (int): The number of hidden units in the classifier.\n",
        "        drop_prob (float): The dropout probability.\n",
        "\n",
        "    Returns:\n",
        "        model (nn.Module): The built neural network.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the pretrained model\n",
        "    if arch == 'vgg13':\n",
        "        model = models.vgg13(weights='IMAGENET1K_V1')  # Use weights argument\n",
        "    elif arch == 'resnet50':\n",
        "        model = models.resnet50(weights='IMAGENET1K_V2')  # Use weights argument\n",
        "    else:\n",
        "        raise ValueError('Invalid architecture. Choose from vgg13 or resnet50.')\n",
        "\n",
        "    # Freeze the pretrained model's parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace the classifier with a custom one\n",
        "    classifier = nn.Sequential(OrderedDict([\n",
        "        ('fc1', nn.Linear(model.classifier[0].in_features, hidden_units)),\n",
        "        ('relu1', nn.ReLU()),\n",
        "        ('dropout1', nn.Dropout(p=drop_prob)),\n",
        "        ('fc2', nn.Linear(hidden_units, 102)),\n",
        "        ('output', nn.LogSoftmax(dim=1))\n",
        "    ]))\n",
        "\n",
        "    model.classifier = classifier\n",
        "\n",
        "    # Move the model to the GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define function to process an image\n",
        "def process_image(image_path):\n",
        "    \"\"\"Processes an image for prediction.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "\n",
        "    Returns:\n",
        "        img (torch.Tensor): The processed image as a PyTorch tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Preprocess the image\n",
        "    img = data_transforms['test'](img)\n",
        "\n",
        "    # Add a batch dimension\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    # Move the image to the GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    img = img.to(device)\n",
        "\n",
        "    return img\n",
        "\n",
        "# Define function to predict the class of an image\n",
        "def predict(image_path, model, topk=5):\n",
        "    \"\"\"Predicts the class of an image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        model (nn.Module): The trained model.\n",
        "        topk (int): The number of top predictions to return.\n",
        "\n",
        "    Returns:\n",
        "        probs (list): A list of probabilities for the top predictions.\n",
        "        classes (list): A list of class indices for the top predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process the image\n",
        "    img = process_image(image_path)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Make the prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "\n",
        "    # Calculate the probabilities and class indices\n",
        "    probs, classes = torch.exp(output).topk(topk)\n",
        "    probs = probs.cpu().numpy()[0]\n",
        "    classes = classes.cpu().numpy()[0]\n",
        "\n",
        "    return probs, classes\n",
        "\n",
        "# Define function to predict an image and print the results\n",
        "def predict_image(image_path, model, class_to_idx, topk=5, category_names=None):\n",
        "    \"\"\"Predicts the class of an image and prints the results.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        model (nn.Module): The trained model.\n",
        "        class_to_idx (dict): A dictionary mapping class indices to class names.\n",
        "        topk (int): The number of top predictions to return.\n",
        "        category_names (dict): A dictionary mapping class indices to category names.\n",
        "    \"\"\"\n",
        "\n",
        "    # Predict the class\n",
        "    probs, classes = predict(image_path, model, topk)\n",
        "\n",
        "    # Create idx_to_class dictionary (reverse mapping)\n",
        "    idx_to_class = {val: key for key, val in class_to_idx.items()}\n",
        "\n",
        "    # Get the class names\n",
        "    class_names = [idx_to_class[class_] for class_ in classes]\n",
        "\n",
        "    # Print the results\n",
        "    print('Top {} predictions:'.format(topk))\n",
        "    for prob, class_name in zip(probs, class_names):\n",
        "        print('  {}: {:.4f}'.format(class_name, prob))\n",
        "\n",
        "    # If category_names is provided, print the category names\n",
        "    if category_names:\n",
        "        print('\\nCategory names:')\n",
        "        for class_name in class_names:\n",
        "            print('  {}'.format(category_names[class_name]))\n",
        "\n",
        "# Define function to parse command line arguments\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Predict flower name from an image')\n",
        "    parser.add_argument('image_path', help='Path to the image file')\n",
        "    parser.add_argument('checkpoint', help='Path to the checkpoint file')\n",
        "    parser.add_argument('--top_k', default=5, type=int, help='Number of top predictions to return')\n",
        "    parser.add_argument('--category_names', default=None, help='Path to a JSON file containing category names')\n",
        "    parser.add_argument('--gpu', action='store_true', help='Use GPU for inference')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "# Define main function\n",
        "def main():\n",
        "    # Parse command line arguments\n",
        "    args = parse_args()\n",
        "\n",
        "    # Set the device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.gpu else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    model, class_to_idx = load_checkpoint(args.checkpoint)\n",
        "\n",
        "    # Load the category names if provided\n",
        "    category_names = None\n",
        "    if args.category_names:\n",
        "        with open(args.category_names, 'r') as f:\n",
        "            category_names = json.load(f)\n",
        "\n",
        "    # Predict the image\n",
        "    predict_image(args.image_path, model, class_to_idx, topk=args.top_k, category_names=category_names)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTtsBU2FkPCL",
        "outputId": "9700fa73-2220-4a3c-a821-cc8c2d84367e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG13_Weights.IMAGENET1K_V1`. You can also use `weights=VGG13_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 3.2050 Acc: 0.3658\n",
            "valid Loss: 1.2407 Acc: 0.6699\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "train Loss: 1.5508 Acc: 0.5894\n",
            "valid Loss: 0.9215 Acc: 0.7543\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "train Loss: 1.4000 Acc: 0.6392\n",
            "valid Loss: 0.8032 Acc: 0.8044\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "train Loss: 1.2699 Acc: 0.6685\n",
            "valid Loss: 0.8978 Acc: 0.7848\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "train Loss: 1.2507 Acc: 0.6806\n",
            "valid Loss: 0.8320 Acc: 0.7885\n",
            "\n",
            "Model trained and saved.\n"
          ]
        }
      ],
      "source": [
        "!python train.py ./flowers --arch vgg13 --learning_rate 0.003 --epochs 5 --gpu --drop_prob 0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py ./flowers/test/1/image_06752.jpg ./checkpoint.pth --top_k 3 --category_names cat_to_name.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMda8RzsokCV",
        "outputId": "e86168a4-ffb6-4f82-fbc7-f0deb23abf3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Top 3 predictions:\n",
            "  1: 0.5319\n",
            "  86: 0.3681\n",
            "  34: 0.0666\n",
            "\n",
            "Category names:\n",
            "  pink primrose\n",
            "  tree mallow\n",
            "  mexican aster\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}